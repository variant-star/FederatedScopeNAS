#[0.0128, 0.984, 0.0032]
wandb:
  use: false  # TODO(Variant)
  name_user: justone
  name_project: distributed_attentive_nas
  client_train_info: True # train results on Client
  server_train_info: True # train results on Server
  online_track: True # origin: false, why
use_gpu: True
use_amp: True

public_data:
  annotated: True

ensemble_distillation:
  enable: true
  type: "avg_logits"  # "avg_logits"

inplace_distillation:
  enable: True
  type: "reverse" # "normal" or "reverse"

overwrite_supernet_with_aggregation: True
freeze_teacher_subnet:
  enable: false
  round_after: 361

server_model_bn_tracking: false
#ensemble_model_bn_tracking: false
supernet_bn_tracking: false

device: 0
#early_stop:
#  patience: 5
seed: 0
federate:
  method: nas_fl
  mode: standalone
  process_num: 1  # maybe we can use gpus >= 1
  client_num: 8
  total_round_num: 360
  sampler: uniform
#  sample_client_rate: 1.0
  make_global_eval: true  # simulate the global eval
  resource_info_file: ''  # TODO(Variant): for personalized NAS
#fedprox:  # TODO(Variant): only for test
#  use: false
#  mu: 0.1
personalization:
  local_param: []
  share_non_trainable_para: true # TODO(Variant): True
data:
  root: data/
  type: cifar10
  splits: [0.8, 0., 0.2]  # TODO(Variant)
  splitter: lda
  splitter_args:
    - alpha: 0.5
#  subsample: 0.05
  consistent_label_distribution: True
  autoaugment: true  # TODO(Variant)
  random_erase: true  # TODO(Variant)
  cutout: true  # TODO(Variant)
#  transform: [['ToTensor'], ['Normalize', {'mean': [0.9637], 'std': [0.1592]}]]
model:
  type: attentive_min_subnet
#  n_classes: 10
  bn_momentum: 0.1
  bn_eps: 0.00001
  drop_out: 0.2  # not work, refer to model_factory.py
  drop_connect: 0.2  # not work, refer to model_factory.py
finetune:
  batch_or_epoch: epoch
  before_eval: false
dataloader:
  type: base
  batch_size: 320
  drop_last: false # only for train dataloader
  shuffle: true # only for train dataloader
  num_workers: 6

# For Client Training:
train:
  local_update_steps: 1
  batch_or_epoch: epoch
  optimizer:
    type: patchSGD
    lr: 0.1
    weight_decay: 0.0005
    weight_decay_bn_bias: 0.
  scheduler:
    type: warmup_cosine_scheduler
    #      max_iters: 360 # iters, based on batch num
    warmup_iters: 5 # epoch * num_batch_per_epoch
    clamp_lr: 0.
    warmup_factor: 0.0001
grad:
  grad_clip: 1.0
criterion:
  type: balanced_softmax # CrossEntropyLoss, balanced_ce
#  label_smoothing: 0
#  beta: 0.9
#  fl_gamma: 2.0
trainer:
  type: ensemble_distillation_trainer
eval:
  freq: 1
  metrics: ['acc']  # 'correct', 'loss_regular'  #, 'avg_loss'
  monitoring: []
  split: ['test'] # 'val',
  best_res_update_round_wise_key: 'test_acc'
  report: ['raw']  # 'weighted_avg', 'avg', 'fairness',

# For Server distillation to knowledge network (training)
server_trainer_specified:
#  type: attentive_min_subnet
  trainer:
    type: ensemble_distillation_trainer
  criterion:
    type: alpha_divergence # pytorch::KLDivLoss
  regularizer:
    mu: 0.0
    type: ''
  dataloader:
    type: base
    batch_size: 320
    drop_last: false # only for train dataloader
    shuffle: true # only for train dataloader
    num_workers: 6
  train:
    round_after: 361
    batch_or_epoch: epoch
    local_update_steps: 1
    optimizer:
      type: patchSGD
      lr: 0.1 # origin: 0.01
      momentum: 0.9
      nesterov: True
      weight_decay: 0.00001
      weight_decay_bn_bias: 0.
    scheduler:
      type: warmup_cosine_scheduler
#      max_iters: 360 # iters, total_round * batch num
      warmup_iters: 5 # epoch * num_batch_per_epoch
      clamp_lr: 0.
      warmup_factor: 0.0001
  grad:
    grad_clip: 1.0
  eval:
    freq: 1
    metrics: ['acc'] #, 'avg_loss'
    monitoring: []
    split:  ['test', 'val', 'train']  # for 'train' split, it has train_transform, "val" is prepared for bn recalibration
    best_res_update_round_wise_key: 'test_acc'
    report: ['raw']  # 'weighted_avg', 'avg', 'fairness',


# For Server distillation to supernet (training)
supernet_trainer_specified:
  trainer:
    type: ensemble_distillation_trainer
  criterion:
    type: alpha_divergence  # alpha_divergence, KLDivLoss,
  regularizer:
    mu: 0.0
    type: ''
  dataloader:
    type: base
    batch_size: 320
    drop_last: false # only for train dataloader
    shuffle: true # only for train dataloader
    num_workers: 6
  train:
    round_after: 0  # TODO(Variant):
    batch_or_epoch: epoch
    local_update_steps: 1  # TODO(Variant)
    optimizer:
      type: patchSGD
      lr: 0.1 # origin: 0.1 # TODO(Variant)
      momentum: 0.9
      nesterov: True
      weight_decay: 0.00001  # TODO(Variant): 0.00001
      weight_decay_bn_bias: 0.
    scheduler:
      type: warmup_cosine_scheduler
#      max_iters: 360 # iters, based on batch num
      warmup_iters: 5 # epoch * num_batch_per_epoch
      clamp_lr: 0.
      warmup_factor: 0.0001
  grad:
    grad_clip: 1.0  # TODO(Variant): not sure
  eval:
    freq: 1
    metrics: ['acc'] #, 'avg_loss'
    monitoring: []
    split: ['test', 'train']  # for 'train' split, it has train_transform
    best_res_update_round_wise_key: 'test_acc'
    report: ['raw']  # 'weighted_avg', 'avg', 'fairness',


  supernet_arch_sampler:
    num_arch_training: 4
    method: 'bestup'
    arch_to_flops_map_file_path: './attentive_nas_data/flops_archs_off_table.map'
    discretize_step: 25
    num_trials: 3


# related supernet_config
supernet:
  type: attentive_supernet
#  n_classes: 10
  bn_momentum: 0
  bn_eps: 0.00001
  drop_out: 0.2
  drop_connect: 0.2
  drop_connect_only_last_two_stages: True



