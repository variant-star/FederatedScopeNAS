use_gpu: true
use_amp: true

consensus:
  type: "avg_logits"

device: 0
seed: 0
model:
  pretrain: ""
federate:
  method: FedMD
  mode: standalone
  process_num: 1  # maybe we can use gpus >= 1
  client_num: 8
  total_round_num: 120
  make_global_eval: true  # simulate the global eval # important!
personalization:
  local_param: []
  share_non_trainable_para: true
data:
  root: data/
  type: cifar100
  splits: [0.8, 0.1, 0.1]  # [all_clients_train_set, all_clients_val_set, server_train_set]
  splitter: lda
  splitter_args:
    - alpha: 0.5
#  subsample: 0.05
  consistent_label_distribution: true
  autoaugment: true
  random_erase: false
  cutout: false
dataloader:
  type: base
  batch_size: 256
  drop_last: false # only for train dataloader
  shuffle: true # only for train dataloader
  pin_memory: false
  num_workers: 6
eval:
  freq: 1
  metrics: ['acc']  # 'correct', 'loss_regular'  #, 'avg_loss'
  monitoring: []
  split: ['test'] # 'val',
  best_res_update_round_wise_key: 'test_acc'
  report: ['raw']  # 'weighted_avg', 'avg', 'fairness',