use_gpu: true
use_amp: true

client_aware_training: True

supernet_arch_sampler:
  num_arch_training: 4
  method: 'bestup'
  arch_to_flops_map_file_path: './attentive_nas_data/flops_archs_off_table.map'
  discretize_step: 25
  num_trials: 3

device: 0
seed: 0
federate:
  method: OneShot
  mode: standalone
  process_num: 1  # maybe we can use gpus >= 1
  client_num: 8
  total_round_num: 120
  make_global_eval: true  # simulate the global eval # important!
personalization:
  local_param: []
  share_non_trainable_para: true
data:
  root: data/
  type: cifar100
  splits: [0.6, 0.2, 0.2]  # [all_clients_train_set, all_clients_val_set, server_train_set]
  splitter: lda
  splitter_args:
    - alpha: 0.5
#  subsample: 0.05
  consistent_label_distribution: true
  autoaugment: true
  random_erase: false
  cutout: false
model:
  type: attentive_supernet
  pretrain: ""
  bn_momentum: 0.1
  bn_eps: 0.00001
  drop_out: 0.2
  drop_connect: 0.2
  drop_connect_only_last_two_stages: true
trainer:
  type: oneshot_supernet_trainer
criterion:
  type: balanced_softmax  # alpha_divergence, kl_divergence(temperature)
dataloader:
  type: base
  batch_size: 256
  drop_last: false # only for train dataloader
  shuffle: true # only for train dataloader
  pin_memory: false
  num_workers: 6
train:
  batch_or_epoch: epoch
  recalibrate_bn: false
  local_update_steps: 1
  optimizer:
    type: patchSGD
    lr: 0.1
    momentum: 0.9
    nesterov: true
    weight_decay: 0.00001
    weight_decay_bn_bias: 0.
  scheduler:
    type: warmup_cosine_scheduler
    # max_iters: 360 # iters, based on batch num
    warmup_iters: 5 # epoch * num_batch_per_epoch
    clamp_lr: 0.
    warmup_factor: 0.0001
grad:
  grad_clip: 1.0
eval:
  freq: 1
  metrics: ['acc'] #, 'avg_loss'
  monitoring: []
  split: ['test', 'train']  # for 'train' split, it has train_transform
  best_res_update_round_wise_key: 'test_acc'
  report: ['raw']  # 'weighted_avg', 'avg', 'fairness',