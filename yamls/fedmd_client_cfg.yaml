client_models_cfg: ""

model:
  type: attentive_subnet
  bn_momentum: 0.1
  bn_eps: 0.00001
  drop_out: 0.2
  drop_connect: 0.2
  drop_connect_only_last_two_stages: true
trainer:
  type: fedmd_trainer
criterion:
  type: balanced_softmax # CrossEntropyLoss, balanced_softmax
  label_smoothing: 0.0
train:
  local_update_steps: 1
  recalibrate_bn: false
  batch_or_epoch: epoch
  optimizer:
    type: patchSGD
    lr: 0.08
    momentum: 0.9
    nesterov: true
    weight_decay: 0.0005  # TODO(Variant): how to set this value
    weight_decay_bn_bias: 0.
  scheduler:
    type: warmup_cosine_scheduler
    # max_iters: 360 # iters, based on batch num
    warmup_iters: 5 # epoch * num_batch_per_epoch
    clamp_lr: 0.
    warmup_factor: 0.0001
grad:
  grad_clip: 1.0
eval:
  freq: 1
  metrics: ['acc']  # 'correct', 'loss_regular'  #, 'avg_loss'
  monitoring: []
  split: ['test'] # 'val',
  best_res_update_round_wise_key: 'test_acc'
  report: ['raw']  # 'weighted_avg', 'avg', 'fairness',
finetune:
  before_eval: false
  batch_or_epoch: epoch
  recalibrate_bn: false
  criterion_base: "eval"  # "train" or "eval"
  local_update_steps: 20
  optimizer:
    type: patchSGD
    lr: 0.01
    momentum: 0.9
    nesterov: true
    weight_decay: 0.00001
    weight_decay_bn_bias: 0.
  scheduler:
    type: warmup_cosine_scheduler
    warmup_iters: 0
    clamp_lr: 0.